{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "336f311a",
   "metadata": {
    "id": "336f311a"
   },
   "source": [
    "# Activity 21.1: Reinforcement Learning Activity\n",
    "\n",
    "Learning Outcomes Addressed:\n",
    "- 5. Implement the Quality matrix and the Bellman equation.\n",
    "- 6. Implement the fundamental steps of reinforcement learning.\n",
    "\n",
    "\n",
    "## Activity Overview\n",
    "\n",
    "In this activity, you will explore how implementing an algorithm using reinforcement learning can improve the performance of your results. In particular, you will work with a Python implementation that simulates the movements of a self-driving cab to calculate how many steps it takes to pick up and drop off a passenger at the correct locations.\n",
    "\n",
    "You will consider the implementations of this problem with and without reinforcement learning and you will be asked to compare the performance, results, and penalties of the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf347ca",
   "metadata": {
    "id": "8cf347ca"
   },
   "source": [
    "## Problem Description\n",
    "\n",
    "Imagine that you want to design a simulation for a self-driving cab (the agent) that wants to pick up a passenger in a certain location and safely drop them off at another one by taking the minimum amount of steps possible.\n",
    "\n",
    "Before you dive deeper into how to implement this problem without and with reinforcement learning, you need to consider and define the following: rewards, states, and actions.\n",
    "\n",
    "#### Rewards\n",
    "\n",
    "- The agent should receive a high positive reward if he drops off the passenger successfully.\n",
    "- The agent should be penalized if it tries to drop off a passenger in the wrong locations.\n",
    "- The agent should get a small negative reward for not making it to the destination after every time-step. In other words, you want to penalize the agent a little if he doesn't make the optimal move.\n",
    "\n",
    "#### State Space\n",
    "\n",
    "Assume that you model the parking lot for this activity as a $5 \\times 5$ grid, which gives you 25 possible taxi locations. Assume further that you can pick up or drop off a passenger at four different locations $(0,0), (0,4), (4,0), (4,3)$. \n",
    "\n",
    "Also, assume that you want to pick-up your passenger at point $S$ and drop him off at point $F$.\n",
    "\n",
    "Take into account one additional state which describes the passenger as being inside of the cab.\n",
    "\n",
    "Therefore, given the dimension of the grid, the total number of possible destinations, and the possible states for the passenger you have a total of $5 \\times 5\\times 4 \\times 5 = 500$ states.\n",
    "\n",
    "#### Action Space\n",
    "\n",
    "Your agent has six possible actions:\n",
    "\n",
    "- Moving South\n",
    "- Moving North\n",
    "- Moving East\n",
    "- Moving West\n",
    "- Picking up the passenger\n",
    "- Dropping off the passenger\n",
    "\n",
    "These six actions define the action space for your agent. Given the map above, you will see that, due to the presence of walls, your agent won't be able to perform some actions. In your code, you will define these walls by providing a -1 penalty for every hit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86970dc",
   "metadata": {
    "id": "b86970dc"
   },
   "source": [
    "## Part 1: Problem Setup\n",
    "\n",
    "The problem described above may seem quite challenging to implement in Python.\n",
    "\n",
    "Luckily, the OpenAi [`gym` *library*](https://gym.openai.com/) already offers an environment that does exactly what you want.\n",
    "\n",
    "In the code cell below, import the *library* and invoke the `\"Taxi-v3\"` environment that already has all the rewards, state spaces, and actions as defined above. You will also set a random seed for reproducibility of the results.\n",
    "\n",
    "Run the code cell below to import and render the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33e8dc07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1384,
     "status": "ok",
     "timestamp": 1645026001916,
     "user": {
      "displayName": "Jessica Cervi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgjfO4C_Hq2VaZB8cT8teoqb8_8Sxq486LcxviJgg=s64",
      "userId": "17758443524817632383"
     },
     "user_tz": 360
    },
    "id": "33e8dc07",
    "outputId": "8b885d06-152d-4604-c98b-e10c476e5c02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+---------+\\n|R: | : :G|\\n| : | : : |\\n| : : : : |\\n|\\x1b[43m \\x1b[0m| : | : |\\n|\\x1b[35mY\\x1b[0m| : |\\x1b[34;1mB\\x1b[0m: |\\n+---------+\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode='ansi')\n",
    "env.reset(seed=0)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc9ad7",
   "metadata": {
    "id": "f9bc9ad7"
   },
   "source": [
    "In the visualization above, the agent (the self-driving cab) is represented by the yellow rectangle. The agent will turn green once the passenger has been picked up.\n",
    "\n",
    "You can also observe that there are four possible pick-up and drop-off locations represented by the letters R (red), G (green), Y (yellow), and B (blue). The code automates the to blue letter represent the passenger pick-up location, and the purple letter to represent the destination.\n",
    "\n",
    "Finally, the pipe (\"|\") represents a wall which the self-driving cab cannot cross.\n",
    "\n",
    "Next, you want to verify that your environment is defined correctly.\n",
    "\n",
    "The code cell below will call the `action_space` *method* to *print* the total number of actions in your problem.\n",
    "\n",
    "Run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f639f7a",
   "metadata": {
    "id": "6f639f7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space {}\".format(env.action_space))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1309c4e",
   "metadata": {
    "id": "f1309c4e"
   },
   "source": [
    "## Question 1\n",
    "\n",
    "Similarly to the previous code cell, complete the *print* *statement* by calling the `observation_space` *method* to *print* the number of states in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff334577",
   "metadata": {
    "id": "ff334577"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b1fb2",
   "metadata": {
    "id": "537b1fb2"
   },
   "source": [
    "Now you can see that your environment has necessary 500 states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218e37e",
   "metadata": {
    "id": "3218e37e"
   },
   "source": [
    "## Question 2\n",
    "\n",
    "Next, you need to encode the position of your agent and the pick-up and drop-off locations in your environment.\n",
    "\n",
    "Observe the visualization of your environment above.\n",
    "\n",
    "In the code cell below, fill in the ellipsis, keeping the following in mind:\n",
    "- The first entry represents the agent row *index*, where each row is numbered from 0 to 4 from top to bottom.\n",
    "- The second entry represents the agent column *index*, where each column is numbered from 0 to 4 from left to right.\n",
    "- The third entry represents the passenger pick-up location. The passenger pick-up locations are encoded in the following way:\n",
    "   - 0: R (red)\n",
    "   - 1: G (green)\n",
    "   - 2: Y (yellow)\n",
    "   - 3: B (blue)\n",
    "   - 4: in taxi\n",
    "- The fourth entry represents the passenger drop-off location. Encode that in a similar way as the pick-up location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "319708c9",
   "metadata": {
    "id": "319708c9"
   },
   "outputs": [],
   "source": [
    "# (taxi row, taxi column, passenger index, destination index)\n",
    "state = env.encode(0, 1, 1, 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbfea9",
   "metadata": {
    "id": "f1bbfea9"
   },
   "source": [
    "Run the code below to generate a number that corresponds to a `state` between 0 and 499."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87a911f5",
   "metadata": {
    "id": "87a911f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 26\n"
     ]
    }
   ],
   "source": [
    "print(\"State:\", state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a008de4",
   "metadata": {
    "id": "6a008de4"
   },
   "source": [
    "You can observe that the original environment corresponds to `state` 26.\n",
    "\n",
    "\n",
    "## The Reward Matrix\n",
    "\n",
    "When you created your environment, the *library* also created an initial reward matrix that has the number of states represented as rows and number of actions represented as columns.\n",
    "\n",
    "## Question 3\n",
    "\n",
    "In the code cell below, fill in the ellipsis with the `state` generated in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aef63005",
   "metadata": {
    "id": "aef63005"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 126, -1, False)],\n",
       " 1: [(1.0, 26, -1, False)],\n",
       " 2: [(1.0, 26, -1, False)],\n",
       " 3: [(1.0, 6, -1, False)],\n",
       " 4: [(1.0, 26, -10, False)],\n",
       " 5: [(1.0, 26, -10, False)]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[26]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1731c514",
   "metadata": {
    "id": "1731c514"
   },
   "source": [
    "The *dictionary* above has the following structure: *`{action: [(probability, nextstate, reward, done)]}`*.\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "- The 0-5 corresponds to the actions (South, North, East, West, pick-up, drop-off) that the agent can perform at your current state in the illustration.\n",
    "- In this environment, `probability` is always 1.0.\n",
    "- The `nextstate` is the state you would be in if you take the action at this *index* of the *dictionary*.\n",
    "- All the movement actions have a `reward` of -1. The pick-up and drop-off actions have a `reward` of -10 in this particular state. If you are in a state where the agent has a passenger and is at the correct destination, you would see a `reward` of 20 at the drop-off action.\n",
    "- The `done` result is used to tell you when you have successfully dropped off a passenger in the correct location.\n",
    "\n",
    "Note that if your agent chooses to explore action two (2) in this state, it would be going East into a wall. The source code has made it impossible to actually move the taxi across a wall, so if the agent chooses that action, it will just keep accumulating -1 penalties, which affects the long-term reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eaf02a",
   "metadata": {
    "id": "d6eaf02a"
   },
   "source": [
    "## Part 2: Python Implementation without Reinforcement Learning\n",
    "\n",
    "In the code cell below, a solution has been implemented to pick up and drop off the passenger without reinforcement learning.\n",
    "\n",
    "Note that you start with the same state (26) that was determined previously.\n",
    "\n",
    "## Question 4\n",
    "\n",
    "Observe the code in the code cell below. What is the condition that will make the infinite `while` *loop* end? This is an open-ended question that requires a written response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab0c64",
   "metadata": {
    "id": "45ab0c64"
   },
   "source": [
    "Question 4: The infinite while loop will end when done is set to True when the taxi drops the passenger at the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485f0e70",
   "metadata": {
    "id": "485f0e70"
   },
   "source": [
    "Next, run the code cell below. Notice that this is a random problem, the number of steps and the number of penalties will change each time you run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "911e1ec7",
   "metadata": {
    "id": "911e1ec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps taken: 55\n",
      "Penalties incurred: 18\n"
     ]
    }
   ],
   "source": [
    "env.s = 26  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties = 0\n",
    "\n",
    "frames = [] \n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    result = env.step(action)\n",
    "    \n",
    "    # Unpack the result according to the new API\n",
    "    state = result[0]               # Observation (state)\n",
    "    reward = result[1]               # Reward\n",
    "    done = result[2]                 # Whether the episode is done\n",
    "    info = result[3] if len(result) > 3 else {}  # Info dictionary if it exists\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "\n",
    "    # Collect frame information\n",
    "    frames.append({\n",
    "        'frame': env.render(),  # render now works without mode\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "    })\n",
    "\n",
    "    epochs += 1\n",
    "\n",
    "print(f\"Steps taken: {epochs}\")\n",
    "print(f\"Penalties incurred: {penalties}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dbd4e4",
   "metadata": {
    "id": "82dbd4e4"
   },
   "source": [
    "Run the code cell below to see your agent in action.\n",
    "\n",
    "Note that this cell will display an number of frames equal to the number of steps returned by the previous code cell.\n",
    "Feel free to stop the code cell below to run; it won't impact the results of your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7dc3c086",
   "metadata": {
    "id": "7dc3c086"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+---------+\\n|R: | : :\\x1b[35mG\\x1b[0m|\\n| : | : : |\\n| : :\\x1b[42m_\\x1b[0m: : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 29\n",
      "State: 257\n",
      "Action: 0\n",
      "Reward: -1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m         sleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m print_frames(frames)\n",
      "Cell \u001b[1;32mIn[97], line 15\u001b[0m, in \u001b[0;36mprint_frames\u001b[1;34m(frames)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m sleep(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "from time import sleep\n",
    "\n",
    "env.reset()\n",
    "def print_frames(frames):\n",
    "    env.reset()\n",
    "    for i, frame in enumerate(frames):\n",
    "        # Display the rendered frame\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(frame['frame'])  # Use the pre-collected frame\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(1)\n",
    "        \n",
    "print_frames(frames)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae68f2",
   "metadata": {
    "id": "54ae68f2"
   },
   "source": [
    "## Part 3: Python Implementation with Reinforcement Learning\n",
    "\n",
    "In the last part of this activity, you will use reinforcement learning and the Quality matrix to optimize your algorithm.\n",
    "\n",
    "## Question 5\n",
    "\n",
    "Define a Quality matrix with all zero entries and dimensions $500 \\times 6$. Assign this matrix to the `q_table` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cfe3e9f6",
   "metadata": {
    "id": "cfe3e9f6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([500,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee8082",
   "metadata": {
    "id": "7bee8082"
   },
   "source": [
    "You can now create the training algorithm that will update this Quality matrix as the agent explores the environment over thousands of episodes.\n",
    "\n",
    "\n",
    "## Question 6\n",
    "\n",
    "Observe the code in the code cell below. What is the main difference between this implementation and the one proposed in Part 2? Which equation is used to train the algorithm and to fill the entries of the Quality matrix? This is an open-ended question that requires a written response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9669289",
   "metadata": {
    "id": "b9669289"
   },
   "source": [
    "Question 6: This implementation uses the random method to choose the next action and updates the quality table to assess and track moves as it progresses through states. It uses the Bellman equation to fill the quality matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac7dda6",
   "metadata": {
    "id": "8ac7dda6"
   },
   "source": [
    "Run the code cell below to train your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a6a2f982",
   "metadata": {
    "id": "a6a2f982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "\n",
      "CPU times: total: 2.53 s\n",
      "Wall time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 10001):\n",
    "    result = env.reset()\n",
    "    \n",
    "    # Unpack the result according to the new API\n",
    "    state = result[0]               # Observation (state)\n",
    "    info = result[1]                # Info dictionary (may contain additional info)\n",
    "    \n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False \n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Exploit learned values\n",
    "\n",
    "        next_result = env.step(action) \n",
    "        \n",
    "        # Ensure the result from step has the expected structure\n",
    "        if len(next_result) < 3:\n",
    "            raise ValueError(f\"Unexpected result from env.step(action): {next_result}\")\n",
    "\n",
    "        # Unpack the result from the step\n",
    "        next_state = next_result[0]      # Next observation (state)\n",
    "        reward = next_result[1]           # Reward\n",
    "        done = next_result[2]             # Whether the episode is done\n",
    "        info = next_result[3] if len(next_result) > 3 else {}  # Info dictionary if it exists\n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "\n",
    "print(\"Training finished.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219cf655",
   "metadata": {
    "id": "219cf655"
   },
   "source": [
    "You can now evaluate the performance of your agent. \n",
    "\n",
    "Notice that now the next action is always selected using the best value from the Quality matrix.\n",
    "\n",
    "Run the code cell below to see how your algorithm performs after picking up and dropping off 10 passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5ba3e7d8",
   "metadata": {
    "id": "5ba3e7d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 10 episodes:\n",
      "Average steps per episode: 12.0\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "passengers = 10\n",
    "\n",
    "for _ in range(passengers):\n",
    "    result = env.reset()  # Unpack the result properly\n",
    "    state = result[0]     # Get the observation (state)\n",
    "    info = result[1]      # Info dictionary (if needed)\n",
    "    \n",
    "    epochs, penalties = 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])  # Select the best action\n",
    "        next_result = env.step(action) \n",
    "        \n",
    "        # Unpack the next result\n",
    "        next_state = next_result[0]          # Next observation (state)\n",
    "        reward = next_result[1]               # Reward\n",
    "        done = next_result[2]                 # Whether the episode is done\n",
    "        info = next_result[3] if len(next_result) > 3 else {}  # Info dictionary if it exists\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state  # Update state\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {passengers} episodes:\")\n",
    "print(f\"Average steps per episode: {total_epochs / passengers}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / passengers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4dce21",
   "metadata": {
    "id": "3b4dce21"
   },
   "source": [
    "## Question 7\n",
    "\n",
    "Compare the performances of the two approaches presented in this activity. Which one performs better? In your summary, include the number of steps taken each time the algorithm ran and the penalties that were incurred. This is an open-ended question that requires a written response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a84733",
   "metadata": {
    "id": "e2a84733"
   },
   "source": [
    "Question 7: The reinforced learning method takes hundreds less steps to reach its goal. Brute force requires about 400 steps with over a dozen penalities while reinforced learning requires less than 15 and no penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8280c6a",
   "metadata": {
    "id": "c8280c6a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Activity21.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
